{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1\n",
    "**(2 points)** Solve the problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min   \\  & x_1^2 + x_2^2\\\\\n",
    "\\text{s.t. } & x_1 + x_2 \\geq 1.\n",
    "\\end{align}\n",
    "$$   \n",
    "by using just the optimality conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution 1.\n",
    "\n",
    "let $x^*$ be a local minimum for constrained problem \n",
    "$$\n",
    "\\begin{align}\n",
    "\\min.  \\quad &f(x) = x_1^2 + x_2^2 \\\\\n",
    "\\text{s.t. } \\quad &g(x) = x_1 + x_2 - 1 \\geq 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let us assume that objective and constraint functions are continuosly differentiable at a point $x^*$ and assume that $x^*$ satisfies some regularity conditions\n",
    "\n",
    "\n",
    "\n",
    "*Lagrangian function*\n",
    "$$\n",
    "\\begin{align}\n",
    "&L(x,\\mu,\\lambda) = f(x)- \\sum_{j=1}^J\\mu_jg_j(x) -\\sum_{k=1}^K\\lambda_kh_k(x) \\\\\n",
    "&L(x,\\mu,\\lambda) = (x_1^2 + x_2^2) - \\mu(x_1 + x_2 -1) \\\\\n",
    "\\end{align}\n",
    "$$.\n",
    "\n",
    "*stationary rule*\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\nabla_xL(x,\\mu,\\lambda) = 0 \\\\\n",
    "&\\nabla_x L(x,\\mu,\\lambda) = (2x_1-\\mu, 2x_2 -\\mu) = 0\\\\\n",
    "\\end{align}\n",
    "$$.\n",
    "\n",
    "*completementary rule*\n",
    "$$\\mu(x_1 + x_2 -1) = 0 $$ \n",
    "\n",
    "*deduction*\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\because 2x_1-\\mu = 0,\\quad 2x_2 -\\mu = 0 \\quad \\rightarrow \\quad \\therefore x_1 = x_2 \\\\\n",
    "& \\because x_1 + x_2 - 1 \\geq 0 \\quad \\rightarrow \\quad \\therefore x_1 > 0, x_2 > 0 \\\\\n",
    "& \\because \\text{(1) and (2)} \\quad \\rightarrow \\quad \\therefore \\mu \\neq 0 \\\\\n",
    "& \\because \\mu(x_1 + x_2 -1) = 0,\\text{and (3)} \\quad \\rightarrow \\quad \\therefore x_1 + x_2 = 1 \\\\\n",
    "& \\because x_1 = x_2,x_1 + x_2 = 1 \\quad \\rightarrow \\quad \\therefore x_1 = 1/2, x_2 = 1/2, \\mu = 1\\\\\n",
    "\\end{align}\n",
    "$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2\n",
    "**(2 points)** Consider a problem\n",
    "$$\\begin{align}\n",
    "\\min   \\  & f(x)\\\\\n",
    "\\text{s.t. } & h_k(x)=0, \\text{ for all } k=1,\\dots,K,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where all the functions are twice differentiable. Show, that the *gradient of the augmented Lagrangian function* is zero in the minimizer $x^*$ of the above problem. In other words, show that $\\nabla_xL_c(x^*,\\lambda^*)=0$, where $\\lambda^*\\in R^n$ is the corresponding optimal Lagrange multiplier vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution 2\n",
    "\n",
    "**augmented Lagrangian function**\n",
    "\n",
    "$$\n",
    "L_c(x,\\lambda) = f(x)+\\lambda h(x)+\\frac12c\\|h(x)\\|^2.\n",
    "$$\n",
    "Above $c\\in \\mathbb R$ is a penalty parameter and $\\lambda \\in \\mathbb R^K$ is a multiplier.\n",
    "\n",
    "**Gradient and Hessian of Augmented Lagrangian:**\n",
    "\n",
    "if $x^*, \\lambda^*$ satisfy the sufficiency condiction of second-order for original problem\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\nabla_{x}L_c(x^*,\\lambda^*) = \\nabla{f(x^*)} + \\nabla{h(x^*)}(\\lambda^*+ch(x^*)) = \\nabla_{x}L(x^*,\\lambda^*) = 0 \\\\\n",
    "&\\nabla^2_{xx}L_c(x^*,\\lambda^*) = \\nabla^2_{xx}L(x^*,\\lambda^*) + c\\nabla h(x^*)^T\\nabla h(x^*)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Second Order Sufficency Condition:**\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\nabla_xL_c(x^*,\\lambda^*)=0 \\\\\n",
    "&y^T\\nabla^2_{xx}L(x^*,\\lambda^*)y > 0, \\forall y \\neq 0, \\nabla{h}(x^*)^Ty = 0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Lemma:**\n",
    "\n",
    "Let P and Q be two symmetric matrices.Assume that Q>=0 and P > 0 on the nullspace of Q, i.e, $x^TPx>0$ for all $x \\neq 0$ with $x^TQx = 0$. Then there exists a scalar C such that\n",
    "\n",
    "$$ P + cQ: \\quad \\text{positive definite} \\quad \\forall c > C$$\n",
    "\n",
    "Then we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&P = \\nabla^2_{xx}L(x^*,\\lambda^*)\\\\\n",
    "&Q = \\nabla h(x^*)^T\\nabla h(x^*) \\\\\n",
    "&\\nabla^2_{xx}L_c(x^*,\\lambda^*) =  P + cQ\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "If c is very large, then solution of unconstrained Augmented Lagrangian x is nearly feasible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### * **For exercises 3-4, we study optimization problem**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\qquad & x_1^2+x_2^2 + x_3^3+(1-x_4)^2\\\\\n",
    "\\text{s.t.}\\qquad &x_1^2+x_2^2-1=0\\\\\n",
    "    &x_1^2+x_3^2-1=0\\\\\n",
    "    &x\\in\\mathbb R^4\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3\n",
    "**(2 points)** Use the SQP method to solve the above problem. **Analyze carefully the result you got!** How does SQP work for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution 3\n",
    "the solution of the quadratic problem with equality constraints\n",
    "$$\n",
    "\\min_p \\frac12 p^T\\nabla^2_{xx}L(x^k,\\lambda^k)p+\\nabla_xL(x^k,\\lambda^k)^Tp\\\\\n",
    "\\text{s.t. }h_j(x^k) + \\nabla h_j(x^k)^Tp = 0. \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KKT Condition**\n",
    "$$\n",
    "\\begin{align}\n",
    "&L(x,\\mu,\\lambda) = f(x)- \\sum_{j=1}^J\\mu_jg_j(x) -\\sum_{k=1}^K\\lambda_kh_k(x) \\\\\n",
    "&L(x,\\mu,\\lambda) = (x_1^2+x_2^2 + x_3^3+(1-x_4)^2) - ( \\lambda_{1}(x_1^2+x_2^2-1) + \\lambda_{2}(x_1^2+x_3^2-1))\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Necessary Condition:**\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\nabla_x L(x^*,\\lambda^*) = 0 \\\\\n",
    "&\\nabla_x L(x_1,\\lambda) = 2x_1 + 2\\lambda_{1}x_1 + 2\\lambda_{2}x_1 \\\\\n",
    "&\\nabla_x L(x_2,\\lambda) = 2x_2 + 2\\lambda_{1}x_2  \\\\\n",
    "&\\nabla_x L(x_3,\\lambda) = 3x_3^2+ 2\\lambda_{2}x_3 \\\\\n",
    "&\\nabla_x L(x_4,\\lambda) = -2(1-x_4)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Hessian Matrix:**\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\nabla^2_{xx}L(x^*,\\lambda^*) = \n",
    "\\begin{bmatrix}\n",
    "2+2\\lambda_{1}+2\\lambda_{2} & 0 & 0 & 0\\\\\n",
    "0& 2+2\\lambda_{1} & 0 & 0  \\\\\n",
    "0& 0 & 6x_3+\\lambda_{2} & 0  \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "the Hessian Matrix is Positive-Definite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized variables: [0.63466484 0.00788769 0.00788769 0.5       ]\n",
      "Objective function value: 0.6528621631738576\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "from autograd import grad, jacobian\n",
    "from autograd import numpy as anp\n",
    "\n",
    "def objective(x):\n",
    "    return x[0]**2 + x[1]**2 + x[2]**3 + (1 - x[3])**2\n",
    "\n",
    "def constraint1(x):\n",
    "    return x[0]**2 + x[1]**2 - 1\n",
    "\n",
    "def constraint2(x):\n",
    "    return x[0]**2 + x[2]**2 - 1\n",
    "\n",
    "# Gradient of the objective\n",
    "grad_obj = grad(objective)\n",
    "\n",
    "# Jacobian of the constraints\n",
    "jac_constraint1 = grad(constraint1)\n",
    "jac_constraint2 = grad(constraint2)\n",
    "\n",
    "# Initial guess\n",
    "x0 = np.array([0.5, 0.5, 0.5, 0.5])\n",
    "\n",
    "# SQP parameters\n",
    "max_iter = 100\n",
    "tol = 1e-6\n",
    "\n",
    "# SQP main loop\n",
    "for i in range(max_iter):\n",
    "    # Compute gradient and Jacobian at current x\n",
    "    g = np.array(grad_obj(x0))\n",
    "    A = np.vstack([jac_constraint1(x0), jac_constraint2(x0)])\n",
    "    \n",
    "    # Solve the QP problem\n",
    "    p = cp.Variable(4)\n",
    "    objective_qp = 0.5 * cp.quad_form(p, np.eye(4)) + g.T @ p\n",
    "    constraints_qp = [A @ (x0 + p) == np.array([1, 1])]\n",
    "    prob = cp.Problem(cp.Minimize(objective_qp), constraints_qp)\n",
    "    prob.solve()\n",
    "    \n",
    "    # Update x\n",
    "    step = p.value\n",
    "    x0 += step\n",
    "    \n",
    "    # Check convergence\n",
    "    if np.linalg.norm(step) < tol:\n",
    "        break\n",
    "\n",
    "print(f\"Optimized variables: {x0}\")\n",
    "print(f\"Objective function value: {objective(x0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 4\n",
    "**(2 points)** Use the augmented Lagrangian method to solve the above problem. **Analyze carefully the result you got!** How does the method work for this problem?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.99999500e-01 -5.74518694e-10  1.40872306e-03  1.00000113e+00]\n",
      "4096.0\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy\n",
    "\n",
    "def f_constrained(x):\n",
    "    return x[0]**2+x[1]**2+x[2]**3+(1-x[3])**2,[],[x[0]**2+x[1]**2-1,x[0]**2+x[2]**2-1]\n",
    "\n",
    "def augmented_langrangian(f,x,la,c):\n",
    "    second_term = float(numpy.matrix(la)*numpy.matrix(f(x)[2]).transpose())\n",
    "    third_term = 0.5*c*numpy.linalg.norm(f(x)[2])**2\n",
    "    return f(x)[0]+second_term+third_term\n",
    "\n",
    "def augmented_langrangian_method(f,start,la0,c0):\n",
    "    x_old = [float('inf')]*2\n",
    "    x_new = start\n",
    "    f_old = float('inf')\n",
    "    f_new = f(x_new)[0]\n",
    "    la = la0\n",
    "    c = c0\n",
    "    steps = []\n",
    "    while abs(f_old-f_new)>0.00001:\n",
    "#    while numpy.linalg.norm(f(x_new)[2])>0.00001: # doesn't work as itself, see starting from any feasible point\n",
    "        res = minimize(lambda x:augmented_langrangian(f,x,la,c),x_new)\n",
    "        x_old = x_new\n",
    "        f_old = f_new\n",
    "        #la = float(la+numpy.matrix(c)*numpy.matrix(f(res.x)[2]).transpose()) # update Lagrangian\n",
    "        la = la+c*numpy.matrix(f(res.x)[2])\n",
    "        #print(la)\n",
    "        x_new = res.x\n",
    "        f_new = f(x_new)[0]\n",
    "        c = 2*c # increase the penalty coefficient\n",
    "        steps.append(list(x_new))\n",
    "    return x_new,c, steps\n",
    "\n",
    "x0 = [0.1,0.1,0.1,1.]\n",
    "#x0 =[10,-5]\n",
    "l0 = [1.0,1.0]\n",
    "c0 = 1.0\n",
    "[x,c,steps_ag] = augmented_langrangian_method(f_constrained,x0,l0,c0)\n",
    "print(x)\n",
    "print(c)\n",
    "print(len(steps_ag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
