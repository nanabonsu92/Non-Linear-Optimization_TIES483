{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "For tasks 1-3, we study optimization problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\qquad & x_1^2+x_2^2 + x_1+2x_2\\\\\n",
    "\\text{s.t.}\\qquad &x_1+x_2=1\\\\\n",
    "    &x_1,x_2\\in\\mathbb R\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objection function: f(x) = x_1^2 + x_2^2 + x_1 + 2x_2\n",
    "def f(x):\n",
    "    return x[0]**2 + x[1]**2 + x[0] + 2*x[1]\n",
    "\n",
    "#equality constrain: h(x) = x_1 + x_2 -1 ==0\n",
    "def h(x):\n",
    "    return x[0]+x[1]-1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **(2 points)** Solve the problem using the penalty function method. Note that it is not sufficient to use a fixed value for $r$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1\n",
    "As the penalty function method, we convert our constraint problem to new function\n",
    "$$\n",
    "\\begin{align}\\\n",
    "\\min \\qquad &Q(x) = f(x)+r\\alpha(x)\\\\\n",
    "\\text{s.t.} \\qquad &x\\in \\mathbb R^n\n",
    "\\end{align}\n",
    "$$\n",
    "f(x) is the object function;\\\n",
    "$\\alpha(x)$ is the penalty function;\n",
    "$$\n",
    "\\begin{align}\\\n",
    "\\min \\qquad &Q(x) = f(x) + r\\sum_{i=1}^{100} h_i(x)^2 + r\\sum_{j=1}^{100} ( \\max(0,g_j(x))^2 )\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\\\n",
    "\\min \\qquad &Q(x) = (x_1^2+x_2^2 + x_1+2x_2) + r(x_1+x_2-1)^2 \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quadratic penalty function: Q(x)= f(x) + r * sigma h_i(x)**2\n",
    "def Q(f,h,r):\n",
    "    return lambda x : f(x) +  r * (h(x)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74345549 0.24345549] 96\n"
     ]
    }
   ],
   "source": [
    "def solve_Q():\n",
    "    epsilon = 10**-4\n",
    "    r = 0\n",
    "    x0 = np.array([float('inf')]*2)\n",
    "    x1 = [0,0]\n",
    "    while np.linalg.norm(x1-x0) > epsilon:\n",
    "        res = minimize(Q(f,h,r), [0, 0],method='BFGS')\n",
    "        x0 = x1\n",
    "        x1 = np.array(res.x)\n",
    "        r = r+1\n",
    "        #print(x1, r)\n",
    "    print(x1,r)\n",
    "\n",
    "solve_Q()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(2 points)** Solve the problem (i.e., approximate the optimal solution) using the barrier function method. Note that you need to do something a bit clever."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2\n",
    "covert the equality to inequality\n",
    "$$\n",
    "\\begin{align}\\\n",
    "\\qquad &g(x)=\n",
    "{ \n",
    "    \\begin{cases}\n",
    "    h(x) + \\epsilon >0 \\\\\n",
    "    \\epsilon - h(x)  <0\n",
    "    \\end{cases}\n",
    "}\\\\\n",
    "\\qquad \\text{s.t.} \\qquad &x\\in \\mathbb R^n\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barrier function method\n",
    "\n",
    "#covert the equality to inequality\n",
    "# g(x) = h(x) + epsilon > 0, epsilon-h(x) < 0\n",
    "def g(x,epsilon):\n",
    "    return [h(x) + epsilon, epsilon-h(x)]\n",
    "# g1 = lambda x: g(x,1)\n",
    "# print(g1([0,0]))\n",
    "def beta(g,x):\n",
    "    try:\n",
    "        value = sum([ 1/max([0,ieq_j]) for ieq_j in g(x)])\n",
    "        #print(value)\n",
    "    except ZeroDivisionError:\n",
    "        value = float('inf')\n",
    "    return value\n",
    "\n",
    "def Q_barrier(f,g,r):\n",
    "    return lambda x : f(x) +  r * beta(g,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74998093 0.24998093] 0.00390625 0.00390625\n"
     ]
    }
   ],
   "source": [
    "import ad\n",
    "def solve_Q_barrier():\n",
    "    r = 1.0\n",
    "    epsilon = 1\n",
    "    x0 = np.array([float('inf')]*2)\n",
    "    x1 = [1,0]\n",
    "    while np.linalg.norm(x1-x0) > 0.0001:\n",
    "        g1 = lambda x: g(x,epsilon)\n",
    "        q = Q_barrier(f,g1,r)\n",
    "        res = minimize(q, x1, method='Newton-CG',\n",
    "        jac=ad.gh(q)[0],hess=ad.gh(q)[1])\n",
    "        x0 = x1\n",
    "        x1 = res.x\n",
    "        r = r/2\n",
    "        epsilon = epsilon/2\n",
    "    print(x1,epsilon,r)\n",
    "\n",
    "solve_Q_barrier()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(2 points)** Solve the problem using projected gradient method. Compare the performance to the penalty function and barrier function methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_vector(A,vector):\n",
    "    #convert A into a matrix\n",
    "    A_matrix = np.matrix(A)\n",
    "    #construct the \"first row\" of the matrix [[I,A^T],[A,0]]\n",
    "    left_matrix_first_row = np.concatenate((np.identity(len(vector)),A_matrix.transpose()), axis=1)\n",
    "    #construct the \"second row\" of the matrix\n",
    "    left_matrix_second_row = np.concatenate((A_matrix,np.matrix(np.zeros([len(A),len(vector)+len(A)-len(A[0])]))), axis=1)\n",
    "    #combine the whole matrix by combining the rows\n",
    "    left_matrix = np.concatenate((left_matrix_first_row,left_matrix_second_row),axis = 0)\n",
    "    #Solve the system of linear equalities from the previous page\n",
    "    return np.linalg.solve(left_matrix, \\\n",
    "                           np.concatenate((np.matrix(vector).transpose(),\\\n",
    "                                           np.zeros([len(A),1])),axis=0))[:len(vector)]\n",
    "# A = [[1,0,0],[0,1,0]]\n",
    "# gradient = [1,1,1]\n",
    "# print(project_vector(A,gradient))\n",
    "def projected_gradient_method(f,A,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        f_old = f_new\n",
    "        gradient = ad.gh(f)[0](x)\n",
    "        grad_proj = project_vector(A,[-i for i in gradient])#The only changes to steepest..\n",
    "        grad_proj = np.array(grad_proj.transpose())[0] #... descent are here!\n",
    "        x = x+grad_proj*step\n",
    "        f_new = f(x)\n",
    "        steps.append(list(x))\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.75, 0.25]), 1.875, [[0.75, 0.25], [0.75, 0.25]])\n"
     ]
    }
   ],
   "source": [
    "res = projected_gradient_method(lambda x:f(x),[[1,1]],[1,0]\\\n",
    "                          ,.5,0.000001)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **(2 points)** Check the *necessary first order KKT conditions* for the solution that you found."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KKT condition validation\n",
    "$$L(x,\\lambda,\\mu) = f(x)- \\sum_{k=1}^K\\mu_kg_k(x) -\\sum_{j=1}^J\\lambda_jh_j(x)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the KKT condition, we need to find the gradient of the Lagrance \n",
    "$$\n",
    "\\begin{align}\n",
    "&L(x,\\lambda,\\mu)  = (x_1^2+x_2^2+x_1+2x_2) + \\lambda_1(x_1+x_2-1)\\\\\n",
    "&\\nabla_xL(x,\\lambda,\\mu) = 0\\\\\n",
    "&\\nabla_xL(x,\\lambda,\\mu) ={ \n",
    "    \\begin{cases}\n",
    "    2x_1+1-\\lambda_1 = 0 \\\\\n",
    "    2x_2+2-\\lambda_1=0\n",
    "    \\end{cases}\n",
    "}\\\\\n",
    "&=> 2(x_1-x_2)-1 = 0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL(x):\n",
    "    return 2*(x[0]-x[1])-1\n",
    "def kkt(x,h):\n",
    "    if abs(dL(x)) <= h:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validataion\n",
    "Since the x* = [0.75,0.25] is the optimum point, so we will check at points from[0.0,0.0] to [1.0,1.0]. with step 0.01, tolerence 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True validated at point: [0.5, 0.0]\n",
      "True validated at point: [0.51, 0.01]\n",
      "True validated at point: [0.52, 0.02]\n",
      "True validated at point: [0.53, 0.03]\n",
      "True validated at point: [0.54, 0.04]\n",
      "True validated at point: [0.55, 0.05]\n",
      "True validated at point: [0.56, 0.06]\n",
      "True validated at point: [0.5700000000000001, 0.07]\n",
      "True validated at point: [0.58, 0.08]\n",
      "True validated at point: [0.59, 0.09]\n",
      "True validated at point: [0.6, 0.1]\n",
      "True validated at point: [0.61, 0.11]\n",
      "True validated at point: [0.62, 0.12]\n",
      "True validated at point: [0.63, 0.13]\n",
      "True validated at point: [0.64, 0.14]\n",
      "True validated at point: [0.65, 0.15]\n",
      "True validated at point: [0.66, 0.16]\n",
      "True validated at point: [0.67, 0.17]\n",
      "True validated at point: [0.68, 0.18]\n",
      "True validated at point: [0.6900000000000001, 0.19]\n",
      "True validated at point: [0.7000000000000001, 0.2]\n",
      "True validated at point: [0.71, 0.21]\n",
      "True validated at point: [0.72, 0.22]\n",
      "True validated at point: [0.73, 0.23]\n",
      "True validated at point: [0.74, 0.24]\n",
      "True validated at point: [0.75, 0.25]\n",
      "True validated at point: [0.76, 0.26]\n",
      "True validated at point: [0.77, 0.27]\n",
      "True validated at point: [0.78, 0.28]\n",
      "True validated at point: [0.79, 0.29]\n",
      "True validated at point: [0.8, 0.3]\n",
      "True validated at point: [0.81, 0.31]\n",
      "True validated at point: [0.8200000000000001, 0.32]\n",
      "True validated at point: [0.8300000000000001, 0.33]\n",
      "True validated at point: [0.84, 0.34]\n",
      "True validated at point: [0.85, 0.35000000000000003]\n",
      "True validated at point: [0.86, 0.36]\n",
      "True validated at point: [0.87, 0.37]\n",
      "True validated at point: [0.88, 0.38]\n",
      "True validated at point: [0.89, 0.39]\n",
      "True validated at point: [0.9, 0.4]\n",
      "True validated at point: [0.91, 0.41000000000000003]\n",
      "True validated at point: [0.92, 0.42]\n",
      "True validated at point: [0.93, 0.43]\n",
      "True validated at point: [0.9400000000000001, 0.44]\n",
      "True validated at point: [0.9500000000000001, 0.45]\n",
      "True validated at point: [0.96, 0.46]\n",
      "True validated at point: [0.97, 0.47000000000000003]\n",
      "True validated at point: [0.98, 0.48]\n",
      "True validated at point: [0.99, 0.49]\n",
      "True validated at point: [1.0, 0.5]\n"
     ]
    }
   ],
   "source": [
    "# h = 0.01\n",
    "# i = np.linspace(0,1,101)\n",
    "# j = np.linspace(0,1,101)\n",
    "# for ik in i:\n",
    "#     for jk in j:\n",
    "#         x0 = [ik,jk]\n",
    "#         if kkt(x0,h):\n",
    "#             print(\"True validated at point:\", x0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True validated at point [0.75, 0.25]\n"
     ]
    }
   ],
   "source": [
    "x = [0.75,0.25]\n",
    "if kkt(x,0.00001):\n",
    "    print(\"True validated at point\", x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc8182279ce6cdaca7fef62ed808d7afcd3782c73d9f961457aec6b24baf3452"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
